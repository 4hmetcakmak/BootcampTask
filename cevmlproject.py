# -*- coding: utf-8 -*-
"""CEVMLProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hX4kMnpZ2vvSEy6w343BcJ6tCfvH5a7m
"""

import pandas as pd
df=pd.read_excel("/content/Yeni Microsoft Excel Çalışma Sayfası.xlsx")
art=df.copy()
art.fillna(0,inplace=True)
art.head()
dummy=pd.get_dummies(art["Choice"])
art=pd.concat((art,dummy), axis=1)
art.head()
art=art.drop(["Choice"],axis=1)
art=art.drop(["k"],axis=1) #y=1 k=0
art=art.drop(0,axis=1)
art.head()
art.rename(columns={"y":"Choice"},inplace=True)
art.head()

art.describe().T

art.info()

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

"""ttf ilk odaklanmada geçen süre, göz bebeğinin kaç kez baktığını, """

a=art[["TFD_Y","TFD_K","TTFF_Y","TTFF_K","FFD_Y",	"FFD_K","TVD_Y","TVD_K","Choice"]]
sns.pairplot(a, hue="Choice", palette="viridis");

plt.scatter("TVD_Y","TVD_K",c="Choice",data=art, cmap="viridis",alpha=0.5); #purple = 0= k    yellow=1=y
plt.xlabel("TVDY")
plt.ylabel("TVDK");

import numpy as np
target=np.asarray(art["Choice"])
data=np.asarray(art.drop("Choice",axis=1))
target.shape
print(data.shape)
print(target.shape)

"""#Plot With PCA"""

from sklearn.decomposition import PCA
pca = PCA(5) 
projected = pca.fit_transform(art.drop("Choice",axis=1))
print(projected)
plt.scatter(projected[:, 0], projected[:, 1],
            c=target, edgecolor='none', alpha=0.5, #indirdiğimizde ne oldu 
            cmap="viridis")
plt.xlabel('TFD')
plt.ylabel('FitCount')
plt.colorbar();

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import PolynomialFeatures
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import validation_curve
from sklearn.model_selection import cross_val_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier as GBC
from lightgbm import LGBMClassifier as LGBMC
from xgboost.sklearn import XGBClassifier as XGB

from sklearn.model_selection import train_test_split
train_data,test_data,train_target,test_target= train_test_split(data,target,random_state=42)

plt.scatter(train_data[:,0],train_data[:,1],c=train_target,cmap="viridis", alpha=0.5);

"""#Naive Bayes Gaussian"""

# Commented out IPython magic to ensure Python compatibility.
modelG=GaussianNB()
modelG1=modelG.fit(train_data,train_target)
# %time modelG.fit(train_data,train_target)
predictG=modelG1.predict(test_data)

print(classification_report(test_target, predictG))
matG = confusion_matrix(test_target, predictG)
sns.heatmap(matG.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#Multinomail Naive Bayes"""

# Commented out IPython magic to ensure Python compatibility.
modelM=MultinomialNB()
modelM1=modelM.fit(train_data,train_target)
# %time modelM.fit(train_data,train_target)
predictM=modelM1.predict(test_data)

print(classification_report(test_target, predictM))
matM = confusion_matrix(test_target, predictM)
sns.heatmap(matM.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#Multinomial Naive Bayes With GridSearchCV"""

# Commented out IPython magic to ensure Python compatibility.
paramM={"alpha":[0.25,0.5,0.75,1], "fit_prior":[True,False]}
gridM=GridSearchCV(modelM,paramM)
gridM1=gridM.fit(train_data,train_target)
print(gridM1.best_params_)
modelM2=gridM1.best_estimator_
modelM3=modelM2.fit(train_data,train_target)
# %time modelM2.fit(train_data,train_target)
predictM1=modelM3.predict(test_data)

print(classification_report(test_target, predictM1))
matM1 = confusion_matrix(test_target, predictM1)
sns.heatmap(matM1.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#Support Vector Machine Kernel RBF"""

# Commented out IPython magic to ensure Python compatibility.
modelsvc=SVC(kernel="rbf",class_weight="balanced")
paramsvc={"C":[1, 5, 10, 50], "gamma":[0.0001, 0.0005, 0.001, 0.005]}
gridsvc=GridSearchCV(modelsvc,paramsvc,cv=10)
gridsvc1=gridsvc.fit(train_data,train_target)
# %time gridsvc.fit(train_data,train_target)
print(gridsvc1.best_params_)
modelsvc1=gridsvc1.best_estimator_
modelsvc2=modelsvc1.fit(train_data,train_target)
predictsvc=modelsvc2.predict(test_data)

print(classification_report(test_target, predictsvc))
matsvc= confusion_matrix(test_target, predictsvc)
sns.heatmap(matsvc.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label'); #0 ları yani k 'ları daha iyi tahmin ediyor ama hala sorun var ve daha uzun sürüyor

"""#Support Vector Machine Kernel Poly"""

# Commented out IPython magic to ensure Python compatibility.
modelsvcW=SVC(kernel="poly",class_weight="balanced")
paramsvcW={"C":[8,10,12,16 ], "gamma":[0.003, 0.005,0.007,0.01],"degree":[3,6,9]}
gridsvcW=GridSearchCV(modelsvc,paramsvcW,cv=10)
gridsvc1W=gridsvcW.fit(train_data,train_target)
# %time gridsvcW.fit(train_data,train_target)
print(gridsvc1W.best_params_)
modelsvc1W=gridsvc1W.best_estimator_
modelsvc2W=modelsvc1W.fit(train_data,train_target)
predictsvcW=modelsvc2W.predict(test_data)

print(classification_report(test_target, predictsvcW))
matsvcW= confusion_matrix(test_target, predictsvcW)
sns.heatmap(matsvcW.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#Random Forest Classifier"""

# Commented out IPython magic to ensure Python compatibility.
modelRF=RandomForestClassifier(class_weight="balanced")
paramRF={"criterion":["gini","entropy"], "bootstrap":[True,False]}
gridRF=GridSearchCV(modelRF,paramRF,cv=10)
GridRF=gridRF.fit(train_data,train_target)
# %time gridRF.fit(train_data,train_target)
print(GridRF.best_params_)
modelRF=GridRF.best_estimator_
ModelRF=modelRF.fit(train_data,train_target)
predictRF=ModelRF.predict(test_data)

print(classification_report(test_target, predictRF))
matRF= confusion_matrix(test_target, predictRF)
sns.heatmap(matRF.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#K-Neighbors Classifier"""

# Commented out IPython magic to ensure Python compatibility.
modelKN=KNeighborsClassifier()
paramKN={"n_neighbors":[2,3,5,8], "weights":["uniform", "distance"],"algorithm": ["auto","ball_tree","kd_tree","brute"],"p":[1,2,3,4]}
gridKN=GridSearchCV(modelKN, paramKN, cv=10)
gridkn=gridKN.fit(train_data,train_target)
# %time gridKN.fit(train_data,train_target)
print(gridkn.best_params_)
modelKN1=gridkn.best_estimator_
modelKN2=modelKN1.fit(train_data,train_target)
predictKN=modelKN2.predict(test_data)

print(classification_report(test_target, predictKN))
matKN= confusion_matrix(test_target, predictKN)
sns.heatmap(matKN.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#Gradient Boosting Classifier"""

# Commented out IPython magic to ensure Python compatibility.
modelGBC=GBC(random_state=42)
paramGBC={"loss":["deviance", "exponential"]} #not learning rate parametresi gittikçe daha küçüğünü veriyor ancak
#tahmin accuracy0.001'den sonra değişmiyor. learning rate overfittinge sebep veriyor 
gridGBC=GridSearchCV(modelGBC,paramGBC, cv=10)
modelGBC1=gridGBC.fit(train_data,train_target)
# %time gridGBC.fit(train_data,train_target)
print(modelGBC1.best_params_)
ModelGBC=modelGBC1.best_estimator_
ModelGBC1=ModelGBC.fit(train_data, train_target)
predictGBC=ModelGBC1.predict(test_data)

print(classification_report(test_target, predictGBC, zero_division=0))
matGBC= confusion_matrix(test_target, predictGBC)
sns.heatmap(matGBC.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#LGBM Classifier """

# Commented out IPython magic to ensure Python compatibility.
modelLGBMC= LGBMC(class_weight="balanced", random_state=42)
paramLGBMC={"reg_alpha":[0.25,0.5,0.75,1],"reg_lambda":[0.25,0.5,0.75,1]}
gridLGBMC=GridSearchCV(modelLGBMC,paramLGBMC,cv=10,)
modelLGBMC=gridLGBMC.fit(train_data,train_target)
# %time gridLGBMC.fit(train_data,train_target)
print(modelLGBMC.best_params_)
ModelLGBMC=modelLGBMC.best_estimator_
ModelLGBMC1=ModelLGBMC.fit(train_data,train_target)
predictLGBMC=ModelLGBMC1.predict(test_data)

print(classification_report(test_target, predictLGBMC))
matLGBMC= confusion_matrix(test_target, predictLGBMC)
sns.heatmap(matLGBMC.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

"""#XGB Classifier"""

# Commented out IPython magic to ensure Python compatibility.
modelXGB=XGB(random_state=42)
paramXGB={"gamma":[0.0001, 0.0005, 0.001, 0.005]}
gridXGB=GridSearchCV(modelXGB,paramXGB,cv=10,)
modelXGB=gridXGB.fit(train_data,train_target)
# %time gridXGB.fit(train_data,train_target)
print(modelXGB.best_params_)
ModelXGB=modelXGB.best_estimator_
ModelXGB1=ModelXGB.fit(train_data,train_target)
predictXGB=ModelXGB1.predict(test_data)

print(classification_report(test_target, predictXGB))
matXGB= confusion_matrix(test_target, predictXGB)
sns.heatmap(matXGB.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

gaussian=accuracy_score(test_target, predictG)
multinomial=accuracy_score(test_target, predictM)
gridmultinomial=accuracy_score(test_target, predictM1)
svcRBF=accuracy_score(test_target, predictsvc)
svcPOLY=accuracy_score(test_target, predictsvcW)
RandomF=accuracy_score(test_target, predictRF)
KNModel=accuracy_score(test_target, predictKN)
GBCac=accuracy_score(test_target, predictGBC)
LGBMCac=accuracy_score(test_target, predictLGBMC)
XGBac=accuracy_score(test_target, predictXGB)
accuracy_list=[gaussian,multinomial,gridmultinomial,svcRBF,svcPOLY,RandomF,KNModel,GBCac,LGBMCac,XGBac]
fig = plt.figure(figsize=(12,4))
ax = fig.add_axes([0,0,1,1])
langs = ["gaussian","multinomial","gridmultinomial","svcRBF","svcPOLY","RandomF","KNModel","GBCac","LGBMCac","XGBac"]
ax.bar(langs,accuracy_list)
ax.set_ylabel("Accuracy Scores")
plt.show()