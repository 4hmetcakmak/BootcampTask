# -*- coding: utf-8 -*-
"""CaliforniaDatasetTask.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZeoLBiWja_PSmodEyeC8HSVIUVAUGPZv
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import pandas as pd
import numpy as np
import tensorflow as tf
from matplotlib import pyplot as plt

# The following lines adjust the granularity of reporting. 
pd.options.display.max_rows = 10
pd.options.display.float_format = "{:.1f}".format

HouseData=pd.read_csv("/content/sample_data/california_housing_train.csv")
HouseData["median_house_value"]/=1000.0 
HouseData.head()

HouseData.describe()

HouseData.corr()

"""#DETECT OUTLIERS


"""

def plot_boxplot(df,ft):
  df.boxplot(column=[ft])
  plt.grid(False)
  plt.show()

plot_boxplot(HouseData,"total_rooms")

"""burada özellikle total roomlarda çok fazla aykırı değer var bunları düzenlemek gerekiyor."""

def outliers(df,ft): #burada outlierları tanımlamak için bir fonksiyon tanımladık 
  Q1=df[ft].quantile(0.25)
  Q3=df[ft].quantile(0.75)
  IQR=Q3-Q1

  LOW= Q1-1.5*IQR
  UP=Q3+1.5*IQR
  
  LS=df.index[(df[ft]<LOW) | (df[ft]>UP)]
  return LS

index_list=[]
for i in ['longitude', 'latitude', 'housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income','median_house_value']:
  index_list.extend(outliers(HouseData,i)) #bu listede tutuyor outliersları

def remove(df,ls):
  ls=sorted(set(ls)) #lsleri düzenledik
  df=df.drop(ls) #dataframeden çıkardık
  return df

HouseDataNew=remove(HouseData,index_list) #bu nesnede tuttum
print(HouseData.shape)
HouseDataNew.shape #yaklaşık 2500 satır gitmiş çok ciddi aykırı değerler varmış

plot_boxplot(HouseDataNew,"total_rooms")

"""#MODEL"""

#@title Define the functions that build and train a model
def build_model(my_learning_rate):
  """Create and compile a simple linear regression model."""
  # Most simple tf.keras models are sequential.
  model = tf.keras.models.Sequential()

  # Describe the topography of the model.
  # The topography of a simple linear regression model
  # is a single node in a single layer.
  model.add(tf.keras.layers.Dense(units=1, 
                                  input_shape=(1,)))

  # Compile the model topography into code that TensorFlow can efficiently
  # execute. Configure training to minimize the model's mean squared error. 
  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),
                loss="mean_squared_error",
                metrics=[tf.keras.metrics.RootMeanSquaredError()])

  return model        


def train_model(model, df, feature, label, epochs, batch_size):
  """Train the model by feeding it data."""

  # Feed the model the feature and the label.
  # The model will train for the specified number of epochs. 
  history = model.fit(x=df[feature],
                      y=df[label],
                      batch_size=batch_size,
                      epochs=epochs)

  # Gather the trained model's weight and bias.
  trained_weight = model.get_weights()[0]
  trained_bias = model.get_weights()[1]

  # The list of epochs is stored separately from the rest of history.
  epochs = history.epoch
  
  # Isolate the error for each epoch.
  hist = pd.DataFrame(history.history)

  # To track the progression of training, we're going to take a snapshot
  # of the model's root mean squared error at each epoch. 
  rmse = hist["root_mean_squared_error"]

  return trained_weight, trained_bias, epochs, rmse

print("Defined the create_model and traing_model functions.")

#@title Define the plotting functions
def plot_the_model(trained_weight, trained_bias, feature, label):
  """Plot the trained model against 10 random training examples."""

  # Label the axes.
  plt.xlabel(feature)
  plt.ylabel(label)

  # Create a scatter plot from 200 random points of the dataset.
  random_examples = HouseDataNew.sample(n=10)
  plt.scatter(random_examples[feature], random_examples[label])

  # Create a red line representing the model. The red line starts
  # at coordinates (x0, y0) and ends at coordinates (x1, y1).
  x0 = 0
  y0 = trained_bias
  x1 = 10000
  y1 = trained_bias + (trained_weight * x1)
  plt.plot([x0, x1], [y0, y1], c='r')

  # Render the scatter plot and the red line.
  plt.show()


def plot_the_loss_curve(epochs, rmse):
  """Plot a curve of loss vs. epoch."""

  plt.figure()
  plt.xlabel("Epoch")
  plt.ylabel("Root Mean Squared Error")

  plt.plot(epochs, rmse, label="Loss")
  plt.legend()
  plt.ylim([rmse.min()*0.97, rmse.max()])
  plt.show()  

print("Defined the plot_the_model and plot_the_loss_curve functions.")

learning_rate = 0.06
epoch = 24
batch_size = 30
my_label="median_house_value"
def ModelTry(my_feature):
  my_model= build_model(learning_rate)
  weight,bias,epochs,rmse=train_model(my_model, HouseDataNew,my_feature,my_label,epoch, batch_size)
  plot_the_model(weight, bias, my_feature, my_label)
  plot_the_loss_curve(epochs,rmse)

"""öncelikle aşağıdaki modeller bir daha çalıştırıldığında farklı görsel çıktılar verebilirler bunun sebebi yukarıdaki random.sample fonksiyonudur"""

for i in ['longitude', 'latitude', 'housing_median_age', 'total_rooms','total_bedrooms', 'population', 'households', 'median_income']:
  ModelTry(i)

"""kendi yaratacağım değişkenlere geçmeden önce bu değişkenler üzerinden yorum yapacak olursam eğer; "median_income", "house_holds" değişkenlerinin model için en iyi değişkenler olduğunu şimdlik söyleyebilirim bunun sebebi ise hata oranlarının az olması ve hata eğrilerinin daha istediğimiz seyirde izlemesi ve tahmin eğrisinin de gerçek değerler ile iyi bir seyirde izlemesi. Şimdi bu datasetindeki değişkenler arası ilişkilere bakarak yapay yani sentetik değişkenler oluşturacağım ve bunlar üzerinden bu işlemleri tekrar yapıp en iyi modeli bulacağım"""

HouseDataNew.corr() #burada korelasyon ilişkileri yüskek olan değişkenleri çeşitli anlamlı işlemler ile birleştirip yeni bir değişken oluşturacağuım

HouseDataNew["rooms_per_person"] = HouseDataNew["total_rooms"] / HouseDataNew["population"]
HouseDataNew["average_person_in_family"]=HouseDataNew["population"]/HouseDataNew["households"]
HouseDataNew["rooms_per_family"]=HouseDataNew["total_rooms"]/HouseDataNew["households"]
HouseDataNew.head()

"""şimdi bu değişkenler üzerinden yapalım bir de """

for i in ["rooms_per_person","average_person_in_family","rooms_per_family"]:
  ModelTry(i)

"""bu sentetik değişkenlerden en iyisi hata oranına bakarsak; "average_person_in_family" değişkeni duruyor ancak curve eğrisi biraz daha geç şekilde aşağı iniyor. "rooms_per_person" değişkeni de güzel bir değişken bu model için hem hata oranı az diğerlerine göre hem de curve eğrisi de istediğimiz şekilde

#SONUÇ

sentetik ve veri setindeki ana değişkenlere baktığımızda aslında en iyi değişkenin "median_income" olduğunu söyleyebiliriz. ben diğer 2 değişken "average_person_in_family" ve "rooms_per_person" değişkenlerini seçerdim çünkü hata oranlar az ve curve eğrileri istediğimiz gibi 0 a yakın bir yerden itibaren azalıyor
"""